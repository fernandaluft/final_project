{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "# Check if the code is running on Google Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    base_path = \"/content/\"\n",
        "    if Path(f\"{base_path}final_project\").is_dir():\n",
        "      %cd {base_path}final_project\n",
        "      !git pull\n",
        "      %cd {base_path}\n",
        "    else:\n",
        "      !git clone https://github.com/fernandaluft/final_project.git\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    base_path = \"/workspaces/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otW-aVRTE_F2",
        "outputId": "b09cf92f-7b50-4f2f-93ef-ef2c8fe928af"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/final_project\n",
            "Already up to date.\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from final_project.src.scraping import Scraping\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import pickle\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "5_qVf-u0E0JD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "986bafd6-7767-4c6c-8ebc-2a16e9350375"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scraping = Scraping(IN_COLAB, sentiment_ds = True)\n",
        "scraping.kaggle_scrape()"
      ],
      "metadata": {
        "id": "GkMOiRBOExvh"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "RWpBeEQgBv4n"
      },
      "outputs": [],
      "source": [
        "class SentimentMLTrain():\n",
        "  def __init__(self, dataset_limit):\n",
        "    self.dataset_limit = dataset_limit\n",
        "    self.stop_words = set(stopwords.words('english'))\n",
        "    self.lemmatizer = WordNetLemmatizer()\n",
        "    self.tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 3))\n",
        "    self.X_train, self.X_test, self.y_train, self.y_test = None, None, None, None\n",
        "    self.best_model = None\n",
        "    with open('/content/final_project/models/tf_idf.pickle', 'wb') as f:\n",
        "      pickle.dump(self.tfidf_vectorizer, f)\n",
        "\n",
        "\n",
        "  def read_sentiment_dataset(self):\n",
        "    !unzip -o -n /content/imdb-dataset-of-65k-movie-reviews-and-translation.zip -d {base_path}final_project/data\n",
        "    os.system(f'rm -rf /content/imdb-dataset-of-65k-movie-reviews-and-translation.zip')\n",
        "    self.sentiment_df = pd.read_csv(f\"{base_path}final_project/data/IMDB-Dataset.csv\").sample(1000)\n",
        "\n",
        "  def preprocess_text(self, text):\n",
        "    text = re.sub(r'\\W|\\d', ' ', str(text))  # Remove special characters and digits\n",
        "    tokens = word_tokenize(text.lower())  # Tokenize and convert to lowercase\n",
        "    lemmatized_tokens = [self.lemmatizer.lemmatize(word) for word in tokens if word not in self.stop_words]  # Lemmatize and remove stopwords\n",
        "    return ' '.join(lemmatized_tokens)\n",
        "\n",
        "  def preprocess_data(self):\n",
        "    self.sentiment_df['clean_text'] = self.sentiment_df['Reviews'].apply(self.preprocess_text)\n",
        "\n",
        "  def split_data(self):\n",
        "    self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
        "        self.tfidf_vectorizer.fit_transform(self.sentiment_df['clean_text']),\n",
        "        self.sentiment_df['Ratings'],\n",
        "        test_size=0.2,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "  def train_model(self):\n",
        "    svm_classifier = SVC(kernel='linear')\n",
        "    param_grid = {'C': [0.1, 1, 10, 100]}  # Hyperparameter grid for tuning\n",
        "    grid_search = GridSearchCV(svm_classifier, param_grid, cv=5)\n",
        "    grid_search.fit(self.X_train, self.y_train)\n",
        "    self.best_model = grid_search.best_estimator_\n",
        "\n",
        "  def evaluate_model(self):\n",
        "    y_pred = self.best_model.predict(self.X_test)\n",
        "    accuracy = accuracy_score(self.y_test, y_pred)\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    print(classification_report(self.y_test, y_pred))\n",
        "\n",
        "  def save_model(self, model_path):\n",
        "    with open(model_path, 'wb') as f:\n",
        "      pickle.dump(self.tfidf_vectorizer, f)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_ml_train = SentimentMLTrain(None)\n",
        "sentiment_ml_train.read_sentiment_dataset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLxku95UIdAZ",
        "outputId": "ca6ab399-fcc0-4fbd-f938-306a1c9a14a8"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "caution:  both -n and -o specified; ignoring -o\n",
            "Archive:  /content/imdb-dataset-of-65k-movie-reviews-and-translation.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_ml_train.sentiment_df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nX8xvMc54H-K",
        "outputId": "5171c66f-fc18-4971-9600-48aae6b15a0b"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 1000 entries, 96255 to 147162\n",
            "Data columns (total 4 columns):\n",
            " #   Column    Non-Null Count  Dtype  \n",
            "---  ------    --------------  -----  \n",
            " 0   Ratings   1000 non-null   float64\n",
            " 1   Reviews   1000 non-null   object \n",
            " 2   Movies    1000 non-null   object \n",
            " 3   Resenhas  1000 non-null   object \n",
            "dtypes: float64(1), object(3)\n",
            "memory usage: 39.1+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_ml_train.preprocess_data()\n",
        "sentiment_ml_train.split_data()\n",
        "sentiment_ml_train.train_model()\n",
        "sentiment_ml_train.evaluate_model()"
      ],
      "metadata": {
        "id": "oYADLwbhJgLh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0941f2b-2ba1-4642-df8d-b01ec2e44587"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.235\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         1.0       0.38      0.32      0.34        19\n",
            "         2.0       0.09      0.08      0.09        12\n",
            "         3.0       0.17      0.33      0.23        18\n",
            "         4.0       0.11      0.09      0.10        23\n",
            "         5.0       0.16      0.31      0.21        16\n",
            "         6.0       0.44      0.17      0.24        24\n",
            "         7.0       0.31      0.12      0.18        32\n",
            "         8.0       0.20      0.18      0.19        17\n",
            "         9.0       0.30      0.35      0.32        17\n",
            "        10.0       0.33      0.45      0.38        22\n",
            "\n",
            "    accuracy                           0.23       200\n",
            "   macro avg       0.25      0.24      0.23       200\n",
            "weighted avg       0.26      0.23      0.23       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_ml_train.save_model(\"/content/final_project/models/model1k.pkl\")"
      ],
      "metadata": {
        "id": "MKqrbdSF4kfr"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -o -n /content/final_project/preprocessed_data/xaa_books_reviews.zip -d {base_path}final_project/data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvIEQAdR50f3",
        "outputId": "0952744e-faee-4950-ddb3-b6eae0d41cd2"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "caution:  both -n and -o specified; ignoring -o\n",
            "Archive:  /content/final_project/preprocessed_data/xaa_books_reviews.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "def calculate_sentiment_book(title):\n",
        "  sentiment_ml = SentimentMLTrain(None)\n",
        "  n_neg = 0\n",
        "  n_pos = 0\n",
        "  with open(\"/content/final_project/models/model1k.pkl\", \"rb\") as f:\n",
        "    model = pickle.load(f)\n",
        "\n",
        "  with open(\"/content/final_project/models/tf_idf.pickle\", \"rb\") as f:\n",
        "    vec = pickle.load(f)\n",
        "  books = pd.read_csv(\"/content/final_project/data/content/final_project/data/books_reviews.csv\")\n",
        "  books_subset = books[books.Title == title]['review/text']\n",
        "  books_subset.info()\n",
        "  for rev, rev2 in books_subset.iteritems():\n",
        "    processed = sentiment_ml.preprocess_text(rev2)\n",
        "    processed = vec.transform(rev2)\n",
        "    score = model.predict(processed)\n",
        "    if score >= 5:\n",
        "      n_pos += 1\n",
        "    else:\n",
        "      n_neg += 1\n",
        "\n",
        "  return [n_neg, n_pos]"
      ],
      "metadata": {
        "id": "JylC5moh6jD8"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(calculate_sentiment_book('Run Baby Run'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 494
        },
        "id": "4hmZCeDP-Coa",
        "outputId": "44d30a7e-c955-4a20-a8ed-a6bcbe4512df"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.series.Series'>\n",
            "Int64Index: 4 entries, 0 to 42256\n",
            "Series name: review/text\n",
            "Non-Null Count  Dtype \n",
            "--------------  ----- \n",
            "4 non-null      object\n",
            "dtypes: object(1)\n",
            "memory usage: 64.0+ bytes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-59-6c5ac5f93ac6>:14: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
            "  for rev, rev2 in books_subset.iteritems():\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotFittedError",
          "evalue": "The TF-IDF vectorizer is not fitted",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-39a15265ccfe>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalculate_sentiment_book\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Run Baby Run'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-59-6c5ac5f93ac6>\u001b[0m in \u001b[0;36mcalculate_sentiment_book\u001b[0;34m(title)\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mrev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrev2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbooks_subset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentiment_ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrev2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrev2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   2153\u001b[0m             \u001b[0mTf\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0midf\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2154\u001b[0m         \"\"\"\n\u001b[0;32m-> 2155\u001b[0;31m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"The TF-IDF vectorizer is not fitted\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2157\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m   1388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1389\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfitted\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1390\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFittedError\u001b[0m: The TF-IDF vectorizer is not fitted"
          ]
        }
      ]
    }
  ]
}