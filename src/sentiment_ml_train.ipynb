{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "# Check if the code is running on Google Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    base_path = \"/content/\"\n",
        "    if Path(f\"{base_path}final_project\").is_dir():\n",
        "      %cd {base_path}final_project\n",
        "      !git pull\n",
        "      %cd {base_path}\n",
        "    else:\n",
        "      !git clone https://github.com/fernandaluft/final_project.git\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    base_path = \"/workspaces/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otW-aVRTE_F2",
        "outputId": "9796adc5-5146-4a79-a9d5-968ab5552a39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'final_project'...\n",
            "remote: Enumerating objects: 126, done.\u001b[K\n",
            "remote: Counting objects: 100% (24/24), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 126 (delta 12), reused 15 (delta 6), pack-reused 102\u001b[K\n",
            "Receiving objects: 100% (126/126), 43.73 MiB | 32.71 MiB/s, done.\n",
            "Resolving deltas: 100% (59/59), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from final_project.src.scraping import Scraping\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "5_qVf-u0E0JD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scraping = Scraping(IN_COLAB, sentiment_ds = True)\n",
        "scraping.kaggle_scrape()"
      ],
      "metadata": {
        "id": "GkMOiRBOExvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWpBeEQgBv4n"
      },
      "outputs": [],
      "source": [
        "class SentimentMLTrain():\n",
        "  def __init__(self, dataset_limit):\n",
        "    self.dataset_limit = dataset_limit\n",
        "\n",
        "  def read_sentiment_dataset(self):\n",
        "    !unzip -o -n /content/imdb-dataset-of-65k-movie-reviews-and-translation.zip -d {base_path}final_project/data\n",
        "    os.system(f'rm -rf /content/imdb-dataset-of-65k-movie-reviews-and-translation.zip')\n",
        "    self.sentiment_df = pd.read_csv(f\"{base_path}final_project/data/IMDB-Dataset.csv\")\n",
        "\n",
        "  def preprocess_text(text):\n",
        "    # remove special chars and digits\n",
        "    text = re.sub(r'\\W|\\d', ' ', text)\n",
        "\n",
        "    #Tokenize the text\n",
        "    tokens = word_tokenize(text.lower())\n",
        "\n",
        "    #Remove stopwords\n",
        "    filtered_tokens = [WordNetLemmatizer().lemmatize(word) for word in tokens if word not in stopwords.words('english')]\n",
        "\n",
        "    #join it all again\n",
        "\n",
        "    ' '.join(filtered_tokens)\n",
        "\n",
        "  def feature_extract_train_test_split(self):\n",
        "    tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,3))\n",
        "\n",
        "    X = tfidf_vectorizer.fit_transform(self.sentiment_df['Reviews'])\n",
        "    y = self.sentiment_df['Ratings']\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_ml_train = SentimentMLTrain(None)\n",
        "sentiment_ml_train.read_sentiment_dataset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLxku95UIdAZ",
        "outputId": "9b27fe59-2012-4f64-cc6d-150cb196315e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "caution:  both -n and -o specified; ignoring -o\n",
            "Archive:  /content/imdb-dataset-of-65k-movie-reviews-and-translation.zip\n",
            "  inflating: /content/final_project/data/IMDB-Dataset.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oYADLwbhJgLh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}